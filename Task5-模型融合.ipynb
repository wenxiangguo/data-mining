{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是 stacking 简单来说 stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。 将个体学习器结合在一起的时候使用的方法叫做结合策略。对于分类问题，我们可以使用投票法来选择输出最多的类。对于回归问题，我们可以将分类器输出的结果求平均值。 上面说的投票法和平均法都是很有效的结合策略，还有一种结合策略是使用另外一个机器学习算法来将个体机器学习器的结果结合在一起，这个方法就是Stacking。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型融合目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多种调参完成的模型进行模型融合。<p>\n",
    "完成对于多种模型的融合，提交融合结果并打卡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单加权融合:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归（分类概率）：算术平均融合（Arithmetic mean),几何平均融合（Geometric mean）<p>\n",
    "分类：投票（Voting) <p>\n",
    "综合：排序融合(Rank averaging)，log融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking/blending:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一般的blending，主要思路是把原始的训练集先分成两部分，比如70%的数据作为新的训练集，剩下30%的数据作为测试集。<p>\n",
    "我们在这70%的数据上训练多个初级学习器，然后用余下的30%预测相应的P。<p>\n",
    "我们直接用这30%数据在第一层预测的P结合真实值，作为新特征继续训练次级学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking工作机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1 先从包含m个样本点的初始数据集，训练出T个初级学习器<p>\n",
    "step2 根据“生成”（交叉验证法/留一法等）的新数据集训练次级学习器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# boosting/bagging（在xgboost，Adaboost,GBDT中已经用到）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting工作机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1  先从初试训练集训练出基学<p>\n",
    "step2  根据基学习器的表现，调整训练样本分布（调整方法见参考链接2），使得基学习器1做错的训练样本在后续得到更多关注<p>\n",
    "step3  根据调整后的样本分布训练基学习器2<p>\n",
    "step4  重复进行,得到 T个基学习器<p>\n",
    "step5  将 T个基学习器加权结合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging工作机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1  基于自助采样法进行采样，可以采样出T个样本集，每个样本集包含m个样本点<p>\n",
    "step2  基于 T个样本集训练得到T个基学习器<p>\n",
    "step3  将T个基学习器结合，分类任务常用简单投票法，回归任务常用简单平均法<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1）简单加权平均，结果直接融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true 代表第模型的真实值\n",
    "y_test_true = [1, 3, 2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]):\n",
    "    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return Weighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred1 MAE: 0.1750000000000001\n",
      "Pred2 MAE: 0.07499999999999993\n",
      "Pred3 MAE: 0.10000000000000009\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# 各模型的预测结果计算MAE\n",
    "print('Pred1 MAE:',metrics.mean_absolute_error(y_test_true, test_pre1))\n",
    "print('Pred2 MAE:',metrics.mean_absolute_error(y_test_true, test_pre2))\n",
    "print('Pred3 MAE:',metrics.mean_absolute_error(y_test_true, test_pre3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_pre MAE: 0.05750000000000027\n"
     ]
    }
   ],
   "source": [
    "## 根据加权计算MAE\n",
    "w = [0.3,0.4,0.3] # 定义比重权值\n",
    "Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)\n",
    "print('Weighted_pre MAE:',metrics.mean_absolute_error(y_test_true, Weighted_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归\\分类概率-融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义结果的加权平均函数\n",
    "def Mean_method(test_pre1,test_pre2,test_pre3):\n",
    "    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1)\n",
    "    return Mean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_pre MAE: 0.06666666666666693\n"
     ]
    }
   ],
   "source": [
    "Mean_pre = Mean_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Mean_pre MAE:',metrics.mean_absolute_error(y_test_true, Mean_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义结果的加权平均函数\n",
    "def Median_method(test_pre1,test_pre2,test_pre3):\n",
    "    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\n",
    "    return Median_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median_pre MAE: 0.07500000000000007\n"
     ]
    }
   ],
   "source": [
    "Median_pre = Median_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking融合(回归):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression()):\n",
    "    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=1).values,y_train_true)\n",
    "    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values)\n",
    "    return Stacking_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一些简单的样本数据，train_regi 代表第i个模型的预测值\n",
    "train_reg1 = [3.2, 8.2, 9.1, 5.2]\n",
    "train_reg2 = [2.9, 8.1, 9.0, 4.9]\n",
    "train_reg3 = [3.1, 7.9, 9.2, 5.0]\n",
    "# y_trian_true 代表train模型的真实值\n",
    "y_train_true = [3, 8, 9, 5] \n",
    "\n",
    "# 生成一些简单的样本数据，test_prei 代表第i个模型的预测值\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true 代表test模型的真实值\n",
    "y_test_true = [1, 3, 2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking_pre MAE: 0.04213483146067476\n"
     ]
    }
   ],
   "source": [
    "model_L2= linear_model.LinearRegression()\n",
    "Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,\n",
    "                               test_pre1,test_pre2,test_pre3,model_L2)\n",
    "print('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1）Voting投票机制："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2）分类的Stacking\\Blending融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本赛题示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# from mlxtend.plotting import plot_learning_curves\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (150000, 335)\n",
      "TestA data shape: (50000, 336)\n"
     ]
    }
   ],
   "source": [
    "Train_data = pd.read_csv('data_train.csv', sep=',')\n",
    "Test_data = pd.read_csv('data_test.csv', sep=',')\n",
    "print('Train data shape:',Train_data.shape)\n",
    "print('TestA data shape:',Test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 402000080.00 MB\n",
      "Memory usage after optimization is: 54450080.00 MB\n",
      "Decreased by 86.5%\n"
     ]
    }
   ],
   "source": [
    "sample_feature = reduce_mem_usage(pd.read_csv('data_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 134400080.00 MB\n",
      "Memory usage after optimization is: 18550080.00 MB\n",
      "Decreased by 86.2%\n"
     ]
    }
   ],
   "source": [
    "test_features=reduce_mem_usage(pd.read_csv('data_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 336)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feature_names = ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_features[continuous_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X.dropna().replace('-', 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=sample_feature['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=sample_feature.drop(['price'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119833, 17), (119833,), (50000, 17))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.59375</td>\n",
       "      <td>5.246094</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>-4.121094</td>\n",
       "      <td>0.737305</td>\n",
       "      <td>0.264404</td>\n",
       "      <td>0.121826</td>\n",
       "      <td>0.070923</td>\n",
       "      <td>0.106567</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>-7.050781</td>\n",
       "      <td>-0.854492</td>\n",
       "      <td>4.800781</td>\n",
       "      <td>0.620117</td>\n",
       "      <td>-3.664062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.332031</td>\n",
       "      <td>0.928223</td>\n",
       "      <td>42.40625</td>\n",
       "      <td>-3.253906</td>\n",
       "      <td>-1.753906</td>\n",
       "      <td>3.646484</td>\n",
       "      <td>-0.725586</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096741</td>\n",
       "      <td>0.013702</td>\n",
       "      <td>0.052368</td>\n",
       "      <td>3.679688</td>\n",
       "      <td>-0.729004</td>\n",
       "      <td>-3.796875</td>\n",
       "      <td>-1.541016</td>\n",
       "      <td>-0.756836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.699219</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>45.84375</td>\n",
       "      <td>4.703125</td>\n",
       "      <td>0.155396</td>\n",
       "      <td>-1.118164</td>\n",
       "      <td>-0.229126</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>0.112061</td>\n",
       "      <td>0.078064</td>\n",
       "      <td>0.062073</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>-4.925781</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.826660</td>\n",
       "      <td>0.138184</td>\n",
       "      <td>0.753906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.082031</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>46.43750</td>\n",
       "      <td>4.320312</td>\n",
       "      <td>0.428955</td>\n",
       "      <td>-2.037109</td>\n",
       "      <td>-0.234741</td>\n",
       "      <td>0.260498</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>0.075989</td>\n",
       "      <td>0.048279</td>\n",
       "      <td>-4.863281</td>\n",
       "      <td>0.505371</td>\n",
       "      <td>1.870117</td>\n",
       "      <td>0.365967</td>\n",
       "      <td>1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.332031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.18750</td>\n",
       "      <td>-3.166016</td>\n",
       "      <td>-1.572266</td>\n",
       "      <td>2.603516</td>\n",
       "      <td>0.387451</td>\n",
       "      <td>0.250977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077820</td>\n",
       "      <td>0.028595</td>\n",
       "      <td>0.081726</td>\n",
       "      <td>3.617188</td>\n",
       "      <td>-0.673340</td>\n",
       "      <td>-3.197266</td>\n",
       "      <td>-0.025681</td>\n",
       "      <td>-0.101318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      power  kilometer       v_0       v_1       v_2       v_3       v_4  \\\n",
       "0  5.750000   1.000000  49.59375  5.246094  1.000977 -4.121094  0.737305   \n",
       "1  4.332031   0.928223  42.40625 -3.253906 -1.753906  3.646484 -0.725586   \n",
       "2  4.699219   0.707031  45.84375  4.703125  0.155396 -1.118164 -0.229126   \n",
       "3  5.082031   0.707031  46.43750  4.320312  0.428955 -2.037109 -0.234741   \n",
       "4  4.332031   1.000000  42.18750 -3.166016 -1.572266  2.603516  0.387451   \n",
       "\n",
       "        v_5       v_6       v_7       v_8       v_9      v_10      v_11  \\\n",
       "0  0.264404  0.121826  0.070923  0.106567  0.078857 -7.050781 -0.854492   \n",
       "1  0.261719  0.000000  0.096741  0.013702  0.052368  3.679688 -0.729004   \n",
       "2  0.260254  0.112061  0.078064  0.062073  0.050537 -4.925781  1.000977   \n",
       "3  0.260498  0.106750  0.081116  0.075989  0.048279 -4.863281  0.505371   \n",
       "4  0.250977  0.000000  0.077820  0.028595  0.081726  3.617188 -0.673340   \n",
       "\n",
       "       v_12      v_13      v_14  \n",
       "0  4.800781  0.620117 -3.664062  \n",
       "1 -3.796875 -1.541016 -0.756836  \n",
       "2  0.826660  0.138184  0.753906  \n",
       "3  1.870117  0.365967  1.312500  \n",
       "4 -3.197266 -0.025681 -0.101318  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feature_names = ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_x[continuous_feature_names]\n",
    "#train_y = train['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.dropna().replace('-', 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(x_train,y_train):\n",
    "    reg_model = linear_model.LinearRegression()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_ridge(x_train,y_train):\n",
    "    reg_model = linear_model.Ridge(alpha=0.8)#alphas=range(1,100,5)\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_lasso(x_train,y_train):\n",
    "    reg_model = linear_model.LassoCV()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_gbdt(x_train,y_train):\n",
    "    estimator =GradientBoostingRegressor(loss='ls',subsample= 0.85,max_depth= 5,n_estimators = 100)\n",
    "    param_grid = { \n",
    "            'learning_rate': [0.05,0.08,0.1,0.2],\n",
    "            }\n",
    "    gbdt = GridSearchCV(estimator, param_grid,cv=3)\n",
    "    gbdt.fit(x_train,y_train)\n",
    "    print(gbdt.best_params_)\n",
    "    # print(gbdt.best_estimator_ )\n",
    "    return gbdt\n",
    "\n",
    "def build_model_xgb(x_train,y_train):\n",
    "    model = xgb.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=5) #, objective ='reg:squarederror'\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def build_model_lgb(x_train,y_train):\n",
    "    estimator = lgb.LGBMRegressor(num_leaves=63,n_estimators = 100)\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "    }\n",
    "    gbm = GridSearchCV(estimator, param_grid)\n",
    "    gbm.fit(x_train, y_train)\n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost的五折交叉回归验证实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mae: 609.9809794734151\n",
      "Val mae 720.139287886939\n"
     ]
    }
   ],
   "source": [
    "xgr = xgb.XGBRegressor(n_estimators=120, learning_rate=0.1, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=7) # ,objective ='reg:squarederror'\n",
    "\n",
    "scores_train = []\n",
    "scores = []\n",
    "\n",
    "## 5折交叉验证方式\n",
    "sk=StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "for train_ind,val_ind in sk.split(x_train,y_train):\n",
    "    \n",
    "    train_x=x_train.iloc[train_ind].values\n",
    "    train_y=y_train.iloc[train_ind]\n",
    "    val_x=x_train.iloc[val_ind].values\n",
    "    val_y=y_train.iloc[val_ind]\n",
    "    \n",
    "    xgr.fit(train_x,train_y)\n",
    "    pred_train_xgb=xgr.predict(train_x)\n",
    "    pred_xgb=xgr.predict(val_x)\n",
    "    \n",
    "    score_train = mean_absolute_error(train_y,pred_train_xgb)\n",
    "    scores_train.append(score_train)\n",
    "    score = mean_absolute_error(val_y,pred_xgb)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Train mae:',np.mean(score_train))\n",
    "print('Val mae',np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集，并用多种方法训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict LR...\n",
      "Predict Ridge...\n",
      "Predict Lasso...\n",
      "Predict GBDT...\n",
      "{'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.3)\n",
    "\n",
    "## Train and Predict\n",
    "print('Predict LR...')\n",
    "model_lr = build_model_lr(x_train,y_train)\n",
    "val_lr = model_lr.predict(x_val)\n",
    "subA_lr = model_lr.predict(test_X)\n",
    "\n",
    "print('Predict Ridge...')\n",
    "model_ridge = build_model_ridge(x_train,y_train)\n",
    "val_ridge = model_ridge.predict(x_val)\n",
    "subA_ridge = model_ridge.predict(test_X)\n",
    "\n",
    "print('Predict Lasso...')\n",
    "model_lasso = build_model_lasso(x_train,y_train)\n",
    "val_lasso = model_lasso.predict(x_val)\n",
    "subA_lasso = model_lasso.predict(test_X)\n",
    "\n",
    "print('Predict GBDT...')\n",
    "model_gbdt = build_model_gbdt(x_train,y_train)\n",
    "val_gbdt = model_gbdt.predict(x_val)\n",
    "subA_gbdt = model_gbdt.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(train_X,train_y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一般比赛中效果最为显著的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Sta_inf(data):\n",
    "    print('_min',np.min(data))\n",
    "    print('_max:',np.max(data))\n",
    "    print('_mean',np.mean(data))\n",
    "    print('_ptp',np.ptp(data))\n",
    "    print('_std',np.std(data))\n",
    "    print('_var',np.var(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict XGB...\n",
      "predict lgb...\n"
     ]
    }
   ],
   "source": [
    "print('predict XGB...')\n",
    "model_xgb = build_model_xgb(x_train,y_train)\n",
    "val_xgb = model_xgb.predict(x_val)\n",
    "subA_xgb = model_xgb.predict(test_X)\n",
    "\n",
    "print('predict lgb...')\n",
    "model_lgb = build_model_lgb(x_train,y_train)\n",
    "val_lgb = model_lgb.predict(x_val)\n",
    "subA_lgb = model_lgb.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sta inf of lgb:\n",
      "_min -326.4078284252571\n",
      "_max: 90876.06238076159\n",
      "_mean 5923.242385425989\n",
      "_ptp 91202.47020918685\n",
      "_std 7370.306693837621\n",
      "_var 54321420.761227645\n"
     ]
    }
   ],
   "source": [
    "print('Sta inf of lgb:')\n",
    "Sta_inf(subA_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型融合很重要，比赛用的比较多的就是stacking和boosting，由于时间紧张，后续要好好理解其中的细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
